t2t commands



#data generation
$t2t-datagen --t2t_usr_dir=/Users/sizhucheng/Desktop/cs224nproject  --data_dir=/Users/sizhucheng/Desktop/cs224nproject/data --problem=mathword_problems

#train
$t2t-trainer --data_dir=/Users/sizhucheng/Desktop/cs224nproject/data --problem=mathword_problems --model=transformer --hparams_set=transformer_prepend --output_dir=Users/sizhucheng/Desktop/cs224nproject --t2t_usr_dir=/Users/sizhucheng/Desktop/cs224nproject

A car is being driven, in a straight line and at a uniform speed, towards the base of a vertical tower. The top of the tower is observed from the car and, in the process, it takes 10 minutes for the angle of e

A car is being driven, in a straight line and at a uniform speed, towards the base of a vertical tower. The top of the tower is observed from the car and, in the process, it takes 10 minutes for the angle of elevation to change from 45\u00b0 to 60\u00b0. After how much more time will this car reach the base of the tower?


#vm
#check tensorflow version
$ pip list | grep tensorflow

$ sudo apt-get install cuda-9-0


# data generation
$ t2t-datagen --t2t_usr_dir=../cs224nfinalproject  --data_dir=../cs224nfinalproject/data --problem=mathword_problems

#train
$ t2t-trainer --data_dir=../cs224nfinalproject/data --problem=mathword_problems --model=transformer --hparams_set=transformer_prepend --output_dir=../cs224nfinalproject/train --t2t_usr_dir=../cs224nfinalproject --worker_gpu=1 --eval_early_stopping_steps=10 --train_steps=100000

#decode
t2t-decoder   --data_dir=../cs224nfinalproject/data   --problem=mathword_problems   --model=transformer   --hparams_set=transformer_prepend   --output_dir=../cs224nfinalproject/train   --decode_hparams="beam_size=4"   --decode_from_file=test.txt   --decode_to_file=translation.eq --t2t_usr_dir=../cs224nfinalproject --worker_gpu=1

Saving dict for global step 49000: global_step = 49000, loss = 1.7458986, metrics-mathword_problems/targets/accuracy = 0.7648953, metrics-mathword_problems/targets/accuracy_per_sequence = 0.49050632, metrics-mathword_problems/targets/accuracy_top5 = 0.8684917, metrics-mathword_problems/targets/approx_bleu_score = 0.79524696, metrics-mathword_problems/targets/neg_log_perplexity = -1.9107592, metrics-mathword_problems/targets/rouge_2_fscore = 0.8364227, metrics-mathword_problems/targets/rouge_L_fscore = 0.8767952

Saving dict for global step 51000: global_step = 51000, loss = 1.7593322, metrics-mathword_problems/targets/accuracy = 0.7700841, metrics-mathword_problems/targets/accuracy_per_sequence = 0.48734176, metrics-mathword_problems/targets/accuracy_top5 = 0.872428, metrics-mathword_problems/targets/approx_bleu_score = 0.7959595, metrics-mathword_problems/targets/neg_log_perplexity = -1.901103, metrics-mathword_problems/targets/rouge_2_fscore = 0.8377965, metrics-mathword_problems/targets/rouge_L_fscore = 0.87691796

Saving dict for global step 100000: global_step = 100000, loss = 1.7508475, metrics-mathword_problems/targets/accuracy = 0.7740204, metrics-mathword_problems/targets/accuracy_per_sequence = 0.49367088, metrics-mathword_problems/targets/accuracy_top5 = 0.87081766, metrics-mathword_problems/targets/approx_bleu_score = 0.8010691, metrics-mathword_problems/targets/neg_log_perplexity = -1.9103955, metrics-mathword_problems/targets/rouge_2_fscore = 0.8421769, metrics-mathword_problems/targets/rouge_L_fscore = 0.8792243

NFO:tensorflow:Loss for final step: 0.00495042.




#2GPU train
$ t2t-trainer --data_dir=../cs224nfinalproject/data --problem=mathword_problems --model=transformer --hparams_set=transformer_base_single_gpu --output_dir=../cs224nfinalproject/traintl --t2t_usr_dir=../cs224nfinalproject --worker_gpu=2 --eval_early_stopping_steps=10 --train_steps=200000

# decode
$ t2t-decoder   --data_dir=../cs224nfinalproject/data   --problem=mathword_problems   --model=transformer   --hparams_set=transformer_base_single_gpu   --output_dir=../cs224nfinalproject/traintl   --decode_hparams="beam_size=4"   --decode_from_file=AQUA_dev   --decode_to_file=translation.eq.dev --t2t_usr_dir=../cs224nfinalproject --worker_gpu=1

INFO:tensorflow:Loss for final step: 0.00084497215

Saving dict for global step 250000: global_step = 250000, loss = 2.2172036, metrics-mathword_problems/targets/accuracy = 0.7362597, metrics-mathword_problems/targets/accuracy_per_sequence = 0.42987806, metrics-mathword_problems/targets/accuracy_top5 = 0.8301365, metrics-mathword_problems/targets/approx_bleu_score = 0.3113331, metrics-mathword_problems/targets/neg_log_perplexity = -2.2437177, metrics-mathword_problems/targets/rouge_2_fscore = 0.4395188, metrics-mathword_problems/targets/rouge_L_fscore = 0.41920754

regex for mathematical expression
(?!(\n))[a-z(\d*\.)?\d+]+[-+/*=\^][a-z(\d*\.)?\d+]+

(?!\n)([\[\(]?[a-z(\d*\.)?\d+]+[ ]?[-+/*=\^x]+[ \(]?[a-z(\d*\.)?\d+]*[\]\)]?)+(?=((\d*\.)?\d+))[a-z(\d*\.)?\d+]+

(?!\n)([\[\(]?[a-z]*((\d*\.)?\d+)*([ ]?[-+/*=\^x]+[ \(]?[a-z]*((\d*\.)?\d+)*[\]\)]?)+(?=((\d*\.)?\d+))([a-z]*((\d*\.)?\d+)*)+



(?<!\n)[\[\(]?[a-z]*((\d*\.)?\d+)*([ ]?[-+/*=\^x]+[ \(]?[a-z]*((\d*\.)?\d+)*[\]\)]?)+(?=((\d*\.)?\d+))([a-z]*((\d*\.)?\d+)*)+

(?<!\n)[\[\(]?([a-z]|((\d*\.)?\d+))([ ]?[-+/*=\^x]+[ \(]?([a-z]|((\d*\.)?\d+))[\]\)]?)+(?=((\d*\.)?\d+))([a-z]|((\d*\.)?\d+)*)+


#training size: 42695




